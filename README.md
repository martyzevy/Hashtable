## Problem 2

To compile, simply type "make" and this will generate an executable called "Hashtable." Running this executable will run a program that tests the birthday paradox and probe rate problem. You can delete this executable by typing "Make clean."

Answer Problem 2 here, **DO NOT FORGET!**

The first test I conducted on my hashtable was confirming the birthday paradox. The birthday paradox states that if you ask 23 people what their birthday is, there is a ~50.7% chance that two of the people surveyed will have the same birthday. This is based on the fact that there are 365 possible birthdays (we are ignoring leap years), and each birthday is equally possible (there is a 1/365 chance of one person having any one particular birthday). This makes sense in the context of a hashtable of size 365, because with a universal hash function and random key insertions, there is a 1/365 chance of a key hashing to one particular index. Hence, a hashtable of size 365 with a universal hash function should have a collision after 23 insertions approximately 50.7% of the time. To confirm this, I initialized a hashtable of size 365 and inserted items with randomly generated keys until there was a collision. I kept track of the number of insertions until the collision occured (or until I had to resize the hashtable, but resizing is very unlikely since this would only occur after 183 insertions as the hashtable resizes when the load factor is greater than 1/2). I repeated this process 10,000 times. I maintained a counter variable that tracked the number of times I inserted less than 24 items before the collision occured. I divided this value by the total amount of trials (10,000) to get the probability of inserting less than 24 items before having a collision, and then multiplied this result by 100 to express it as a percentage. I found that 53.08% of the time, I inserted 23 or fewer items before a collision. The expected result was 50.07%, so my result is reasonablly close (it is only off by 2.38%). I believe there are two reasons for this small discrepency. The first reason is that I am unable to perfectly randomize the keys. To generated random strings, I randomly generated a number between 1 and 26 28 times (so each string was length 28), each time adding the corresponding letter to the string. However, randomly generated numbers are imperfect in C++, so my random string generation is imperfect. The other reason I didn't get exactly 50.7% is because my hash function may not be perfect. While my hash function is designed to distribute keys equally across all of the indices, it is likely not 100% effective in doing so. The expected value of 50.7% from the birthday paradox assumes there is a 1/365 chance that someones birthday is any one particular day of the year. Because my random key generation is imperfect and because my hash function might not be perfect in assigning a 1/365 chance of a key hashing to any one particular index, my results were slightly different than 50.7%. However, the result I got — 53.08% — is very close to the expected value and indeed demonstrates the birthday paradox, and that roughly half the time, my hashtable will have a collision after 23 insertions. 

For the probe rate problem, I attempted to confirm that on average, after n insertions into my hashtable, there should be 2n/3 probes. My hashtable resizes when adding an element would push the load factor over 1/2, and when it resizes, it roughly doubles in size (the size doubles and then increments by one until it is at a prime number). Therefore, after a resize the load factor changes from ~1/2 to ~1/4. Initially, my hashtable goes from load factor 0 to load factor ~1/2, but after the first resize it will oscillate between a load factor of ~1/2 and ~1/4. When the load factor is 1/2, there should be 1 probe on average, and when it is 1/4, there should be 1/3 probes on average. It makes intuitive sense the average number of probes is higher when the load factor is higher because this means there are less empty slots. Assuming the hashtable spends an equal amount of time at a load factor of 1/4 and 1/2, the overall average number of probes should be the average between the number of probes when the load factor is 1/2 and the number of probes when the load factor is 1/4. The average between 1 and 1/3 is 2/3. Therefore, after n insertions the average number of probes should be 2n/3. Additionally, I attempted to confirm the insertion with the most probes should have log(n) probes. To test these expected results, I inserted n items into my hashtable and added up the number of probes after each insertion. I also kept track of the largest insertion. I did this 1000 times and calculated the average number of total probes and the average largest insertion. I did this for several different values of n (50, 100, 200, 400, 800). For each value of n, I reported the expected value (2n/3), as well as my calculated average total probes. I also reported the expected largest probe log(n), and the value I calculated for the average largest probe. My results were very close to the expected results. When n = 400, the expected number of probes was 266.667, and the value I calculated was 266.707. This is virtually the same number. When n was higher (800), my results were slighly less accurate to the expected result. Here, the expected number of total probes is 533.333 but I got 544.678. Similarly, when n was lower (100) the expected result was 66.6667 and I got 61.382. Even though I had some discrepency at these values, I was still reasonably close to the expected number. Furthermore, for the largest probe aspect of the problem, my results were also reasonably close to the expected results. When n = 400, I expected to get 8.64 and I got 7.49. Similarly, when n = 200, I expected to get 7.64 for the largest probe and I got 6.437. Regaurdless of the value of n, my values were roughly 1 away from the expected result, which I believe is reasonably close. The values may be slightly off due to imperfections in random key generation and hash function imperfections. There is randomness involved in hashtable collisions, and even when you take the average of 1000 trials, there can be imperfect results. For the discrepencies within the total probes aspect, there are a few factors that could explain why I was sometimes slightly off for certain values of n. I always used the same initial size for my hashtable (11), regaurdless of the number of items I was inserting. Therefore, for some values of n, I had to resize a different number of times. The amount of time I spent at a load factor of ~1/4 and the amount of time I spent at load factor ~1/2 was different for different values of n, but the expected value 2n/3 is based on the hashtable spending an equal amount of time at both these load factors. If the hashtable does not spend 50% of the time at load factor 1/2 and 50% of the time at load factor 1/4, the results will be different from 2n/3. Despite these issues, I was still able to get an average total number of probes very close to 2n/3 for all values of n and was able to get an average largest probe very close to log(n) for all n. This confirms that with quadratic probing, the largest probe grows logarithmically with n, and the total number of probes grows linearly with n.  

